"""
ASRç®¡ç†æ¨¡å— - ä¸“æ³¨äº Sherpa-ONNX å¼•æ“çš„æç®€é©±åŠ¨
ä¸å†åŒ…å«å†—ä½™çš„æ ‡ç‚¹æ¨¡å‹é€»è¾‘å’Œå¤æ‚çš„æ­£åˆ™å¯å‘å¼ç®—æ³•

ä¿®å¤ï¼šåœ¨ä¸»è¿›ç¨‹ä¸­è§£ææ¨¡å‹è·¯å¾„åä¼ é€’ç»™å­è¿›ç¨‹ï¼Œé¿å…å­è¿›ç¨‹è·¯å¾„è§£æé—®é¢˜
"""

import os
import re
import gc
import sys
import numpy as np
import multiprocessing
import traceback
from abc import ABC, abstractmethod
from typing import Optional, List
from PyQt6.QtCore import QObject, pyqtSignal, QThread, pyqtSlot

from model_config import (
    get_model_config, 
    ASREngineType, 
    ASROutputMode
)

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè§£å†³å¯èƒ½çš„OpenMPåº“å†²çª
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

def clean_asr_output(text: str, mode: str = "raw", is_insertion: bool = False) -> str:
    """
    æ¸…ç†ASRè¾“å‡ºæ–‡æœ¬
    mode: "raw" ä»…åŸºç¡€æ¸…ç†æ ‡ç­¾; "cleaned" é¢å¤–æ‰§è¡Œæ­£åˆ™å‡€åŒ–
    is_insertion: å¦‚æœä¸º Trueï¼Œåˆ™å‰¥ç¦»æœ«å°¾å¥å·ï¼›å¦‚æœä¸º Falseï¼Œåˆ™ä¿ç•™ã€‚
    """
    if not text:
        return text
        
    # 1. åŸºç¡€æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰æ¨¡å‹å†…ç½®æ ‡ç­¾ <|xxx|> å’Œ [xxx]
    text = re.sub(r'<\|.*?\|>', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    
    # 2. åŸºç¡€æ ‡ç‚¹ä¼˜åŒ– (æ— è®ºä»€ä¹ˆæ¨¡å¼éƒ½æ‰§è¡Œ)
    # A. æ™ºèƒ½æ ‡ç‚¹å¤„ç† (Smart Punctuation)
    base_markers = r'ç„¶å|ä½†æ˜¯|å¯æ˜¯|é‚£ä¸ª|å—¯|å‘ƒ|å¸Œæœ›ä»–|è§‰å¾—ä»–|çš„è¯|çš„æ—¶å€™|è€Œä¸”|å°±æ˜¯|å…¶å®|æ‰€ä»¥|åªæ˜¯|ä¸è¿‡|å› ä¸º|æ‰€ä»¥|æˆ–è€…|å¹¶ä¸”|æ‰€ä»¥è¯´|æˆ–è€…æ˜¯|æ¯”å¦‚è¯´'
    
    # è·å–å­¦ä¹ åˆ°çš„è§„åˆ™
    try:
        cfg = get_model_config()
        learned = cfg.get_learned_markers_regex()
        if learned:
            incomplete_markers = f'({base_markers}|{learned})[ã€‚ï¼ï¼Ÿ]$'
        else:
            incomplete_markers = f'({base_markers})[ã€‚ï¼ï¼Ÿ]$'
    except:
        incomplete_markers = f'({base_markers})[ã€‚ï¼ï¼Ÿ]$'

    # é€»è¾‘ 1: å¤„ç†å¤šå¥é€»è¾‘ - â€œç•™é€—å»å¥â€
    # å¦‚æœæ£€æµ‹åˆ°å†…éƒ¨å¥å·ï¼Œå°†å…¶æ›¿æ¢ä¸ºé€—å· (ç”¨æˆ·åé¦ˆï¼šä¸¤å¥è¯ä¹‹é—´çš„é€—å·è¿˜æ˜¯éœ€è¦)
    if text:
        # A. æŸ¥æ‰¾æ‰€æœ‰å¥å·ï¼Œå¦‚æœåé¢è¿˜æœ‰æ–‡å­—ï¼Œåˆ™å°†å…¶æ›¿æ¢ä¸ºé€—å·
        text = re.sub(r'ã€‚(?!$)', 'ï¼Œ', text)
        
        # B. å¤„ç†æœ«å°¾å¥å·
        # B. å¤„ç†æœ«å°¾å¥å·
        if is_insertion:
            # æ’å…¥æ¨¡å¼ï¼šå½»åº•å‰¥ç¦»æœ«å°¾å¥å· (åŒ…æ‹¬å…¨è§’å’ŒåŠè§’)
            text = text.rstrip('ã€‚ï¼ï¼Ÿ.?!')
        else:
            # éæ’å…¥æ¨¡å¼ (æ–°èµ·ä¸€æ®µ æˆ– è¿½åŠ )ï¼š
            # å¦‚æœè¯†åˆ«ç»“æœæœ¬æ¥æ²¡æœ‰å¥å·ï¼Œå¼ºåˆ¶è¡¥å…¨
            # å¿…é¡»æ£€æŸ¥å…¨è§’å’ŒåŠè§’æ ‡ç‚¹ï¼Œé˜²æ­¢ "test." å˜æˆ "test.ã€‚"
            if text and not (text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'))):
                # åªæœ‰å½“å®ƒä¸åƒæ˜¯ä¸€ä¸ªæœªå®Œæˆçš„å¥å­æ—¶æ‰åŠ 
                if not re.search(incomplete_markers, text):
                    text += "ã€‚"

    # é€»è¾‘ 2: å¤„ç†æ˜¾å¼çš„â€œæœªå®Œæˆâ€æ ‡è¯†è¯ (æ— è®ºæ˜¯å¦æ’å…¥éƒ½å»æ‰æ ‡ç‚¹)
    if re.search(incomplete_markers, text):
        text = text.rstrip('ã€‚ï¼ï¼Ÿ')
        
    # é€»è¾‘ 3: çŸ­æ–‡æœ¬ç‰‡æ®µæ·±åº¦ä¿æŠ¤ (å¦‚æœæ˜¯æ’å…¥æ¨¡å¼ä¸”çŸ­è¯­ï¼Œæ›´å€¾å‘äºå»æ‰æ‰€æœ‰ç»“å°¾æ ‡ç‚¹)
    if is_insertion:
        core_text = text.rstrip('ã€‚ï¼ï¼Ÿ')
        if core_text and len(core_text) <= 5:
            sentence_particles = r'.*[äº†å—å§å‘¢å•Šå‘€å“‡å˜›å“’å–”å–½å“©]$|.*[ã€‚ï¼Œï¼ï¼Ÿ]$|.*[0-9a-zA-Z]$'
            if not re.match(sentence_particles, core_text):
                text = core_text

    # å¼ºåˆ¶ç§»é™¤è¿ç»­é‡å¤æ ‡ç‚¹ (ä¾‹å¦‚ "ã€‚ã€‚" -> "ã€‚" æˆ– ".ã€‚" -> "ã€‚")
    text = re.sub(r'([ã€‚ï¼Œï¼ï¼Ÿ.?!])\1+', r'\1', text)

    # 2. å¦‚æœæ˜¯"æ­£åˆ™è¡¨è¾¾ (Cleaned)"æ¨¡å¼ï¼Œæ‰§è¡Œæ›´æ¿€è¿›çš„å‡€åŒ–
    if mode == ASROutputMode.CLEANED.value:
        # C. (å·²ç§»é™¤) å¼ºåˆ¶ä¸­æ—¥è‹±æ–‡æ··æ’ç©ºæ ¼ä¼˜åŒ– - å“åº”ç”¨æˆ·åé¦ˆç§»é™¤
        # text = re.sub(r'([\u4e00-\u9fa5])([a-zA-Z0-9])', r'\1 \2', text)
        # text = re.sub(r'([a-zA-Z0-9])([\u4e00-\u9fa5])', r'\1 \2', text)
        
        # D. ç§»é™¤å¥é¦–å¥å°¾çš„ç©ºç™½å­—ç¬¦
        text = text.strip()
    
    # E. Emoji æ¨¡å¼
    try:
        from model_config import EmojiMode, get_model_config
        # é‡æ–°è·å–é…ç½®ä»¥ç¡®ä¿æœ€æ–°
        cfg = get_model_config()
        mode = cfg.emoji_mode
        
        if mode == EmojiMode.TRIGGER.value:
            # è¯­éŸ³è§¦å‘æ¨¡å¼ï¼šæ£€æµ‹å¥æœ«å…³é”®è¯å¹¶æ›¿æ¢
            triggers = {
                "ç¬‘å“­": "ğŸ˜‚", "å“ˆå“ˆ": "ğŸ˜„", "å¼€å¿ƒ": "ğŸ˜Š", 
                "ç‚¹èµ": "ğŸ‘", "æ˜Ÿæ˜Ÿ": "ğŸŒŸ", "çˆ±å¿ƒ": "â¤ï¸", 
                "ç–‘é—®": "â“", "ç”Ÿæ°”": "ğŸ˜ ", "æµæ³ª": "ğŸ˜­",
                "é¼“æŒ": "ğŸ‘", "åº†ç¥": "ğŸ‰", "åˆå": "ğŸ™",
                "åŠ æ²¹": "ğŸ’ª", "æ»‘ç¨½": "ğŸ¤ª", "æ€è€ƒ": "ğŸ¤”"
            }
            # æ£€æŸ¥å¥æœ« (å¿½ç•¥æœ€åçš„æ ‡ç‚¹)
            # å…ˆå‰¥ç¦»æ ‡ç‚¹
            content = text
            suffix = ""
            if content and content[-1] in "ã€‚ï¼Œï¼ï¼Ÿ":
                suffix = content[-1]
                content = content[:-1]
                
            for k, v in triggers.items():
                if content.endswith(k):
                    # ç§»é™¤å…³é”®è¯
                    prefix = content[:-len(k)]
                    # ç§»é™¤å…³é”®è¯å‰é¢çš„æ ‡ç‚¹ (å¦‚ "æœ‰é“ç†ï¼Œ" -> "æœ‰é“ç†")
                    if prefix.endswith(("ï¼Œ", "ã€‚")):
                        prefix = prefix[:-1]
                    
                    content = prefix + v
                    # è§¦å‘æ¨¡å¼ä¸‹ï¼ŒEmoji è§†ä½œå¥æœ«ï¼Œä¸å†è¿½åŠ åŸæœ‰çš„å¥å°¾æ ‡ç‚¹
                    text = content 
                    break

        elif mode == EmojiMode.AUTO.value:
            # è‡ªåŠ¨æ¨¡å¼ï¼šæ ¹æ®è¯­æ°”è¯æ·»åŠ ï¼Œé»˜è®¤ç¬‘å“­
            # æƒ…æ„Ÿå…³é”®è¯æ˜ å°„ï¼ˆç®€åŒ–çš„å…³é”®è¯åˆ—è¡¨ï¼‰
            sentiment_map = {
                "ğŸ˜„": ["å“ˆå“ˆ", "å˜¿å˜¿", "å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å¥½ç¬‘"],
                "ğŸ˜Š": ["ä½ å¥½", "è°¢è°¢", "æ”¶åˆ°", "å¥½çš„", "æ²¡é—®é¢˜", "å–œæ¬¢"],
                "ğŸ‘": ["ä¸é”™", "å‰å®³", "ç‰›", "èµ", "æ”¯æŒ", "é¡ºåˆ©"],
                "ğŸ˜­": ["éš¾è¿‡", "ä¼¤å¿ƒ", "å‘œå‘œ", "æƒ¨", "ç—›è‹¦"],
                "ğŸ˜ ": ["è®¨åŒ", "çƒ¦", "æ»š", "æ°”æ­»"],
                "ğŸ™": ["æ‹œæ‰˜", "éº»çƒ¦", "æ„Ÿè°¢", "è¾›è‹¦"],
                "ğŸ¤”": ["è§‰å¾—", "æƒ³", "å¯èƒ½", "æ˜¯å¦", "ä¸ºä»€ä¹ˆ"],
                "ğŸ˜‚": [] # Default fallback
            }
            
            found_emoji = None
            for emoji, keywords in sentiment_map.items():
                for kw in keywords:
                    if kw in text:
                        found_emoji = emoji
                        break
                if found_emoji: break
            
            if not found_emoji:
                found_emoji = "ğŸ˜‚"
            
            # å¦‚æœåŸæ–‡ä»¥å¥å·æˆ–é€—å·ç»“å°¾ï¼Œå…ˆç§»é™¤ï¼Œå†åŠ  Emoji
            if text.endswith(("ã€‚", "ï¼Œ")):
                text = text[:-1]
            text += found_emoji
            
    except Exception as e:
        print(f"[ASRManager] Emoji error: {e}")

    # 4. ç§»é™¤å¤šä½™çš„å¤šé‡ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ===== ONNX æ¨ç†ç‹¬ç«‹è¿›ç¨‹æ ¸å¿ƒå‡½æ•° =====
def onnx_inference_worker(model_path, input_queue, output_queue, log_file=None):
    """
    ç‹¬ç«‹çš„ ONNX æ¨ç†è¿›ç¨‹
    
    é‡è¦ï¼šmodel_path å¿…é¡»æ˜¯åœ¨ä¸»è¿›ç¨‹ä¸­å·²è§£æå¥½çš„ç»å¯¹è·¯å¾„
    """
    try:
        import sys
        if log_file:
            try:
                import os
                sys.stdout = open(log_file, "a", encoding="utf-8")
                sys.stderr = sys.stdout
            except: pass

        import sherpa_onnx
        print(f"[ASR-Proc] æ­£åœ¨åŠ è½½ Sherpa-ONNX æ¨¡å‹: {model_path}")
        print(f"[ASR-Proc] æ¨¡å‹è·¯å¾„å­˜åœ¨: {os.path.exists(model_path)}")
        
        # å®šä¹‰æ ¸å¿ƒæ–‡ä»¶
        model_file = os.path.join(model_path, "model.int8.onnx")
        if not os.path.exists(model_file):
            model_file = os.path.join(model_path, "model.onnx")
            
        tokens_file = os.path.join(model_path, "tokens.txt")
        
        print(f"[ASR-Proc] æ¨¡å‹æ–‡ä»¶: {model_file}, å­˜åœ¨: {os.path.exists(model_file)}")
        print(f"[ASR-Proc] Tokensæ–‡ä»¶: {tokens_file}, å­˜åœ¨: {os.path.exists(tokens_file)}")
        
        if not os.path.exists(model_file) or not os.path.exists(tokens_file):
            raise FileNotFoundError(f"æ ¸å¿ƒæ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ {model_path} ç›®å½•")

        # åˆå§‹åŒ–è¯†åˆ«å™¨
        recognizer = sherpa_onnx.OfflineRecognizer.from_sense_voice(
            model=model_file,
            tokens=tokens_file,
            use_itn=True,  # å¯ç”¨å†…ç½®ITNä»¥æ¢å¤æ ‡ç‚¹
            language="auto", # æ˜¾å¼æŒ‡å®šè‡ªåŠ¨æ£€æµ‹
            num_threads=4
        )
            
        print(f"[ASR-Proc] æ¨¡å‹åŠ è½½æˆåŠŸ")
        sys.stdout.flush()
        
        # é€šçŸ¥ä¸»è¿›ç¨‹å·²å°±ç»ª
        output_queue.put(("ready", True))
        
        while True:
            # ç­‰å¾…ä»»åŠ¡
            task = input_queue.get()
            if task is None: 
                break
            
            try:
                # è½¬åŒ–æ•°æ®
                audio_data = np.array(task, dtype=np.float32)
                
                # Sherpa æç®€æ¨ç†æµç¨‹
                stream = recognizer.create_stream()
                stream.accept_waveform(16000, audio_data)
                recognizer.decode_stream(stream)
                
                # ç›´æ¥è¿”å›è¯†åˆ«ç»“æœï¼Œç”±ä¸»è¿›ç¨‹æ ¹æ®æ¨¡å¼è¿›è¡Œæ¬¡çº§æ¸…ç†
                text = stream.result.text
                output_queue.put(("result", text))
                
            except Exception as e:
                # åœ¨æ‰“åŒ…åçš„æ— æ§åˆ¶å°æ¨¡å¼ä¸‹ï¼Œsys.stdout å¯èƒ½ä¸º None
                try: print(f"[ASR-Proc] è½¬å†™ä¸­é”™è¯¯: {e}")
                except: pass
                output_queue.put(("result", ""))
            
    except Exception as e:
        err = f"ASRè¿›ç¨‹å´©æºƒ: {str(e)}"
        try: 
            print(err)
            traceback.print_exc()
        except: pass
        output_queue.put(("fatal", err))
    finally:
        try: print("[ASR-Proc] è¿›ç¨‹å·²é€€å‡º")
        except: pass


# ===== æ ¸å¿ƒå¼•æ“ä»£ç† =====
class OnnxASREngine:
    def __init__(self):
        self.is_loaded = False
        self.input_queue = multiprocessing.Queue()
        self.output_queue = multiprocessing.Queue()
        self.process = None
    
    def load(self, model_path: str) -> bool:
        """
        åŠ è½½ ASR æ¨¡å‹
        
        é‡è¦ï¼šmodel_path å¿…é¡»æ˜¯å·²è§£æå¥½çš„ç»å¯¹è·¯å¾„
        """
        try:
            if self.process and self.process.is_alive():
                self.unload()
            
            # éªŒè¯æ¨¡å‹è·¯å¾„
            if not model_path or not os.path.exists(model_path):
                print(f"[ASR-Engine] æ¨¡å‹è·¯å¾„æ— æ•ˆ: {model_path}")
                return False
            
            # è·å–æ—¥å¿—ç›®å½•
            from model_config import get_model_config
            cfg = get_model_config()
            log_file = os.path.join(cfg.DATA_DIR, "asr_process.log")

            print(f"[ASR-Engine] å¯åŠ¨ ASR è¿›ç¨‹ï¼Œæ¨¡å‹è·¯å¾„: {model_path}")

            self.process = multiprocessing.Process(
                target=onnx_inference_worker,
                args=(model_path, self.input_queue, self.output_queue, log_file),
                daemon=True
            )
            self.process.start()
            
            # ç­‰å¾…ç¡®è®¤ä¿¡å·
            try:
                msg_type, val = self.output_queue.get(timeout=30)
                if msg_type == "ready":
                    self.is_loaded = True
                    print(f"[ASR-Engine] ASR è¿›ç¨‹å°±ç»ª")
                    return True
                elif msg_type == "fatal":
                    print(f"[ASR-Engine] ASR è¿›ç¨‹å¯åŠ¨å¤±è´¥: {val}")
            except Exception as e:
                print(f"[ASR-Engine] ç­‰å¾… ASR è¿›ç¨‹è¶…æ—¶: {e}")
            return False
        except Exception as e:
            print(f"[ASR-Engine] å¯åŠ¨å¼‚å¸¸: {e}")
            return False
    
    def transcribe(self, audio_data) -> str:
        if not self.is_loaded or not self.process.is_alive():
            return ""
        try:
            audio_list = audio_data.tolist() if isinstance(audio_data, np.ndarray) else audio_data
            self.input_queue.put(audio_list)
            msg_type, val = self.output_queue.get(timeout=30)
            return val
        except:
            return ""

    def unload(self):
        if self.process and self.process.is_alive():
            try:
                self.input_queue.put(None)
                self.process.join(timeout=3)
                if self.process.is_alive(): 
                    self.process.terminate()
                    self.process.join(timeout=1)
            except: pass
            finally:
                try:
                    self.input_queue.close()
                    self.output_queue.close()
                except: pass
        self.process = None
        self.is_loaded = False
        self.input_queue = multiprocessing.Queue()
        self.output_queue = multiprocessing.Queue()

# ===== ASR Worker & Manager =====
class ASRWorker(QObject):
    model_ready = pyqtSignal()
    result_ready = pyqtSignal(str)
    error_occurred = pyqtSignal(str)
    status_changed = pyqtSignal(str)
    
    def __init__(self):
        super().__init__()
        self.config = get_model_config()
        self.engine = OnnxASREngine()
    
    @pyqtSlot()
    def load_model(self):
        # åœ¨ä¸»è¿›ç¨‹ä¸­è§£ææ¨¡å‹è·¯å¾„
        model_path = self.config.get_asr_model_path()
        
        if not model_path:
            self.error_occurred.emit("æœªæ‰¾åˆ°è¯­éŸ³è¯†åˆ«æ¨¡å‹")
            return
            
        if not os.path.exists(model_path):
            self.error_occurred.emit(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return
        
        self.status_changed.emit(f"æ­£åœ¨å¯åŠ¨è¯­éŸ³å¼•æ“...")
        print(f"[ASRWorker] è§£æçš„æ¨¡å‹è·¯å¾„: {model_path}")
        
        if self.engine.load(model_path):
            self.status_changed.emit("è¯­éŸ³å¼•æ“å·²å°±ç»ª")
            self.model_ready.emit()
        else:
            self.error_occurred.emit("è¯­éŸ³å¼•æ“åŠ è½½å¤±è´¥")
    
    @pyqtSlot(object, bool)
    def transcribe(self, audio_data, is_insertion=False):
        if not self.engine.is_loaded: return
        try:
            raw_text = self.engine.transcribe(audio_data)
            if raw_text:
                mode = self.config.asr_output_mode
                cleaned_text = clean_asr_output(raw_text, mode=mode, is_insertion=is_insertion)
                self.result_ready.emit(cleaned_text)
        except:
            pass

class ASRManager(QObject):
    _instance = None
    _initialized = False
    
    model_ready = pyqtSignal()
    result_ready = pyqtSignal(str)
    error = pyqtSignal(str)
    status_changed = pyqtSignal(str)
    
    _sig_load_model = pyqtSignal()
    _sig_transcribe = pyqtSignal(object, bool)
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(ASRManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not ASRManager._initialized:
            super().__init__()
            ASRManager._initialized = True
            self.worker = ASRWorker()
            self.thread = QThread()
            self.worker.moveToThread(self.thread)
            
            self.worker.model_ready.connect(self.model_ready.emit)
            self.worker.result_ready.connect(self.result_ready.emit)
            self.worker.error_occurred.connect(self.error.emit)
            self.worker.status_changed.connect(self.status_changed.emit)
            self._sig_load_model.connect(self.worker.load_model)
            self._sig_transcribe.connect(self.worker.transcribe)
            self.thread.start()

    def start(self): self._sig_load_model.emit()
    
    def transcribe_async(self, audio_data, is_insertion=False):
        data = audio_data.tolist() if isinstance(audio_data, np.ndarray) else audio_data
        self._sig_transcribe.emit(data, is_insertion)
    
    def cleanup(self):
        if self.thread.isRunning():
            self.thread.quit()
            self.thread.wait()
        if self.worker.engine: self.worker.engine.unload()
